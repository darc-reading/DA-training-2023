{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51815d54",
   "metadata": {},
   "source": [
    "# Practical 3: EnKF in L96\n",
    "\n",
    "The objective of this practical is to perform DA experiments in the Lorenz-96 model with an ensemble Kalman\n",
    "filter testing out the impact of the type of Kalman filter, inflation and localisation on the DA performance.\n",
    "\n",
    "The differential equations of the Lorenz-96 model are given as\n",
    "\\begin{equation}\n",
    "\\frac{\\partial x_{n}}{\\partial t} = (x_{n+1}-x_{n-2})x_{n-1} - x_{i} + F\n",
    "\\end{equation}\n",
    "with $0 \\leq n < N_{x}$, and $x_{n}(t)$ assumed to be periodic, e.g. $x_{N_{x}}(t)=x_{0}(t)$. \n",
    "\n",
    "Let's start with importing all functions that will be used in this practical. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e35ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from tools.L96_model import lorenz96\n",
    "from tools.obs import createH, gen_obs\n",
    "from tools.enkf import kfs\n",
    "from tools.diag import rmse_spread\n",
    "from tools.plots import plotRMSP, plotL96, plotDA_kf, plot_LocMatrix, plotL96obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48bebb5",
   "metadata": {},
   "source": [
    "## Nature run\n",
    "\n",
    "This section generates the nature run of the experiment, i.e. what we consider to be\n",
    "the truth. You can change the initial condition `x0`, the final time `tmax` (consider that the model time\n",
    "step is 0.025 time units), the number of variables `Nx` and the forcing `F` in the model. For the benefit of speed\n",
    "and in order to display figures in an easier manner, we will use `Nx = 12`.\n",
    "\n",
    "This model can be run from any given initial condition, but the default is to spin it up from a\n",
    "perturbation around the unstable fixed point of the system $x_{n}(t)=F\\,\\forall n$. \n",
    "You will get a Hovmoller diagram (a contour plot showing the time evolution of the different variables \n",
    "in a circle of latitude), as well as a figure with $N_{x}$ panels. \n",
    "\n",
    "We also create a new initial condition that is different from the truth in `xguess`. \n",
    "In the end, we want data assimilation to bring the model run started off from this perturbed initial condition \n",
    "closer to the truth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728cbbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'L96' #Model to be used. \n",
    "Nx = int(12) # number of state variables. Must be multiple of 4.\n",
    "F = 8 #Forcing\n",
    "x0 = np.array([0.,.05] + (Nx-2)*[0.]) + F # let it spin from rest (x_n(t=0) = F, forall n apart n=1)\n",
    "tmax = 8 # The final time of the nature run simulation\n",
    "discard = 150 #Spinup period to discard from output. \n",
    "dt = 0.025 #Time step integration scheme.\n",
    "\n",
    "#Create truth.\n",
    "t = np.arange(0, tmax+dt, dt)\n",
    "xt = lorenz96(x0, tmax, dt, discard, F)\n",
    "\n",
    "#Plot truth.\n",
    "plotL96(t, np.array(xt), Nx, model)\n",
    "\n",
    "#Initial conditions different from truth.\n",
    "np.random.seed(100)\n",
    "xguess = xt[:,0] + np.random.normal(size=np.shape(xt[:,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c8d53c",
   "metadata": {},
   "source": [
    "## Data assimilation\n",
    "\n",
    "Next, we will create the experiment class. Objects of this class hold the settings and results for our \n",
    "DA experiments. An experiment using default settings can be created with `exp = Experiment()`. After creation, \n",
    "the experiment can be run using `exp.run()` and different\n",
    "plotting operations of the results can be carried out (e.g. `exp.plot_state()`, `exp.plot_metrics()`). The default\n",
    "settings can be overwritten by passing them to the initialiser. E.g. `exp = Experiment(n_ens=12)` creates an\n",
    "experiment that uses the default settings for all parameters except the ensemble size `n_ens`. This has been changed\n",
    "from the default 24 members to 12 members. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50789be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class Experiment:\n",
    "    \"\"\"\n",
    "    Class that holds all settings for a single Lorenz-96 ensemble Kalman filter experiment. \n",
    "    \n",
    "    Any setting can be overwritten by passing the new setting as key,value-pair to the constructor. \n",
    "    \n",
    "    Methods \n",
    "    -------\n",
    "    create_observations\n",
    "        Sample observations from truth. \n",
    "    run\n",
    "        Run the DA model and store the output. \n",
    "    calculate_metrics\n",
    "        Calculate performance metrics.\n",
    "    plot_metrics\n",
    "        Plot the metrics produced by self.calculate_metrics as function of time. \n",
    "    plot_state\n",
    "        Plot ensemble mean for forecast and analysis ensemble together with truth and observations \n",
    "        as function of time. \n",
    "    plot_localisation\n",
    "         Plot the localisation matrix.\n",
    "        \n",
    "    Attributes\n",
    "    ----------\n",
    "    x0 : np.ndarray \n",
    "        Initial model state. Default is true initial condition.\n",
    "    seed : int\n",
    "        Seed for random number generator used to create observational errors\n",
    "    period_obs : int>=1\n",
    "        Number of time steps between observations\n",
    "    obs_grid: 'all' | '0101' | 'landsea' | 'foot_cent' | 'foot_6'\n",
    "        Observation operator to be used. \n",
    "    var_obs: float>0\n",
    "        Observational error variance. \n",
    "    footprint : int \n",
    "        If obs_grid='foot_6' the length-scale of the observation foot print. \n",
    "    n_ens: int>=1\n",
    "        Number of ensemble members.\n",
    "    da_method: 'SEnKF' | 'ETKF'\n",
    "        Ensemble Kalman method to be used. \n",
    "    inflation: float\n",
    "        Ensemble inflation. If inflation=0 no ensemble inflation is applied. \n",
    "    loc_method: 'CG' | 'cutoff'\n",
    "        Localisation method.\n",
    "    loc_radius: NoneType | float>0\n",
    "        Half-width localisation radius in grid points. If None, no localisation is applied. \n",
    "        \n",
    "    Xb: 3D np.ndarray   \n",
    "        Ensemble of background states with time along the 0th axis, grid position along the 1st axis and \n",
    "        ensemble member along the 2nd axis. \n",
    "    xb: 2D np.ndarray \n",
    "        Background ensemble mean with time along the 0th axis and grid position along the 1st axis. \n",
    "    Xa: 3D np.ndarray   \n",
    "        Ensemble of analysis states with time along the 0th axis, grid position along the 1st axis and \n",
    "        ensemble member along the 2nd axis. \n",
    "    xa: 2D np.ndarray \n",
    "        Analysis ensemble mean with time along the 0th axis and grid position along the 1st axis. \n",
    "    L_obs: 2D np.ndarray \n",
    "        Array with localisation coefficients between elements in the state and observations.\n",
    "    L_x: 2D np.ndarray \n",
    "        Array with localisation coefficients between elements in the state.\n",
    "    tobs: 1D np.ndarray \n",
    "        Array with observation times. \n",
    "    y: 2D np.ndarray \n",
    "        Array with ith column containing observed values at time self.tobs[i]\n",
    "    R: 2D np.ndarray \n",
    "        Observation error covariance matrix. \n",
    "    n_obs: int \n",
    "        Number of observations per observation time step. \n",
    "    observed_vars: list of int\n",
    "        Indices of the state variables that are at one time or another are observed. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #Default model settings. \n",
    "    x0: np.ndarray = dataclasses.field(default_factory=lambda:np.array(x0))\n",
    "        \n",
    "    #Default observation operator settings.\n",
    "    seed: int = 1 \n",
    "    period_obs: int = 10\n",
    "    obs_grid: str = 'all' \n",
    "    var_obs: float = 2.0 \n",
    "    footprint: int = None\n",
    "        \n",
    "    #Default data assimilation system settings.\n",
    "    n_ens: int = 24\n",
    "    da_method: str = 'SEnKF' \n",
    "    inflation: float = 0.0\n",
    "        \n",
    "    #Localization settings\n",
    "    loc_method: str = 'GC' \n",
    "    loc_radius: float = None\n",
    "        \n",
    "    def create_observations(self):\n",
    "        \"\"\" Sample observations from truth. \"\"\"\n",
    "        self.n_obs, self.H = createH(self.obs_grid, Nx, self.footprint)\n",
    "        self.observed_vars = [ivar for ivar,is_observed in enumerate(np.any(self.H, axis=0)) if is_observed]\n",
    "        self.tobs, self.y, self.R = gen_obs(t, xt, self.period_obs, self.H, self.var_obs, self.seed)\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\" Run the DA model and store the output. \"\"\"\n",
    "        #Create observations.\n",
    "        self.create_observations()\n",
    "        \n",
    "        #State background/analysis\n",
    "        self.Xb, self.xb, self.Xa, self.xa, self.L_obs, self.L_x = kfs(self.x0, float(F), lorenz96, t, self.tobs, self.y,\n",
    "                                                                       self.H, self.R, self.inflation, \n",
    "                                                                       self.n_ens, self.da_method, \n",
    "                                                                       lam=self.loc_radius,\n",
    "                                                                       loctype=self.loc_method,\n",
    "                                                                       back0='random', desv=1.0,\n",
    "                                                                       seed=self.seed)\n",
    "        \n",
    "        #cast \n",
    "        self.L_obs, self.L_x = np.array(self.L_obs), np.array(self.L_x)\n",
    "        \n",
    "    def calculate_metrics(self, step):\n",
    "        \"\"\" \n",
    "        Calculate performance metrics.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        step : int \n",
    "            Number of time steps between states for which metrics will be calculated. \n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        m : xr.Dataset object\n",
    "            Dataset containing time series of RMSE and ensemble spread.\n",
    "        \n",
    "        \"\"\"\n",
    "        m = xr.Dataset(coords = {'DA':(['DA'],['background','analysis']), \n",
    "                                 'time':(['time'], t[::step])}\n",
    "                      )\n",
    "        \n",
    "        #Initialise\n",
    "        m['rmse'] = (['DA','time'], np.zeros((2,len(t[::step]))) )\n",
    "        m['spread'] = (['DA','time'], np.zeros((2,len(t[::step]))) )\n",
    "        \n",
    "        #Background metrics\n",
    "        m['rmse'][0], m['spread'][0] = rmse_spread(xt, self.xb, self.Xb, step)\n",
    "        \n",
    "        #Analysis metrics\n",
    "        m['rmse'][1], m['spread'][1] = rmse_spread(xt, self.xa, self.Xa, step)\n",
    "\n",
    "        return m\n",
    "    \n",
    "    def plot_metrics(self, step):\n",
    "        \"\"\"\n",
    "        Plot the metrics produced by self.calculate_metrics as function of time. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        step : int \n",
    "            Number of time steps between states for which metrics will be calculated. \n",
    "            \n",
    "        \"\"\"\n",
    "        m = self.calculate_metrics(step)\n",
    "        plotRMSP(str(self), t, m['rmse'].sel(DA='background').data, m['rmse'].sel(DA='analysis').data,\n",
    "                 m['spread'].sel(DA='background').data, m['spread'].sel(DA='analysis').data)\n",
    "        \n",
    "        \n",
    "    def plot_state(self):\n",
    "        \"\"\"\n",
    "        Plot ensemble mean for forecast and analysis ensemble together with truth and observations \n",
    "        as function of time. \n",
    "        \"\"\"\n",
    "        plotDA_kf(t, xt, self.tobs, self.H, self.y, self.Xb, self.xb, self.Xa, self.xa, str(exp))\n",
    "   \n",
    "    def __str__(self):\n",
    "        \"\"\" Name of the experiment. \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        String with name of experiment. \n",
    "        \n",
    "        \"\"\"\n",
    "        return ('ob freq:'+str(self.period_obs)+', density:'+str(self.obs_grid)+\n",
    "                 ', err var:'+str(self.var_obs)+', M='+str(self.n_ens)+', loc_radius='+str(self.loc_radius)\n",
    "                +', inflation='+str(self.inflation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e837380",
   "metadata": {},
   "source": [
    "Next we create \"artificial\" observations from the truth we just generated. This is done by applying the \n",
    "linear operator $\\mathbf{H}$ to the state $x(t)$\n",
    "$\\mathbf{H}$ can take on many shapes, e.g. $\\mathbf{H}$ can observe all variables (`obs_grid='all'`),\n",
    "only the even variables (`obs_grid='1010'`), or the first 6 variables (`obs_grid='landsea'`).\n",
    "\n",
    "The state will not necessarily have to be observed at every time step. You can set the number of time steps between\n",
    "observations with the variable `period_obs`.\n",
    "    \n",
    "Finally, we add some white random noise to our samples of the nature run. This noise represents the \n",
    "observational errors from e.g. instrument errors. The observational error covariance matrix $\\mathbf{R}$ is \n",
    "assumed to be diagonal (common assumption), but you can set the observational variance (i.e. the values on \n",
    "the diagonal of $\\mathbf{R}$) with `var_obs`. \n",
    "\n",
    "In summary, the observations at time $t$, $y(t)$, are given as $y(t) = \\mathbf{H}x(t) + \\epsilon(t)$. Here\n",
    "$\\epsilon(t)$ is a random realisation from the normal distribution $\\mathcal{N}(0,\\mathbf{R})$. The settings\n",
    "to generate observations together with a plot of them are created in the next cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022e6bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Experiment()\n",
    "exp.create_observations()\n",
    "\n",
    "#Plot the truth together with observations. \n",
    "plotL96obs(exp.tobs, exp.y, exp.n_obs, str(exp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564f0f9a",
   "metadata": {},
   "source": [
    "## State estimation\n",
    "\n",
    "In this section we will set the observational standard deviation to\n",
    "$\\sigma_{obs}=\\sqrt{2}$, and look at the effect of varying the following: the observational frequency `period_obs`, \n",
    "the observation density `obs_grid`, the ensemble size `n_ens`. \n",
    "\n",
    "### Stochastic ensemble Kalman filter\n",
    "First, we will try to assimilate the observations using the stochastic ensemble Kalman filter (`da_method='SEnKF'`). \n",
    "For reference, in the stochastic ensemble Kalman filter the analysis of the $n$th ensemble member, $x^{a,(n)}$, \n",
    "i.e. the ensemble member after the application of DA, is given by \n",
    "\n",
    "\\begin{equation}\n",
    "    x^{a,(n)} = x^{b,(n)} + \\mathbf{K}(y - \\mathbf{H}x^{b,(n)} + \\mathbf{R}^{\\frac{1}{2}}\\epsilon)\n",
    "\\end{equation}\n",
    "\n",
    "Here $x^{b,(n)}$ is the model state in the $n$th ensemble member before DA at a time $t$, \n",
    "$\\epsilon$ a realisation from the normal distribution $\\mathcal{N}(0,\\mathbf{R})$ and \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{K} = \\mathbf{B} \\mathbf{H}^{\\rm{T}} (\\mathbf{H} \\mathbf{B} \\mathbf{H}^{\\rm{T}} + \\mathbf{R})^{-1}\n",
    "\\end{equation}\n",
    "\n",
    "the Kalman gain matrix with background error covariance\n",
    "\\begin{equation}\n",
    "\\mathbf{B} = \\frac{1}{N_{ens}-1} \\sum_{n=1}^{N_{ens}} (x^{b,(n)}-\\overline{x^{b}}) \n",
    "(x^{b,(n)}-\\overline{x^{b}})^{\\rm{T}}\n",
    "\\end{equation}\n",
    "\n",
    "and $\\overline{x^{b}} = \\frac{1}{N_{ens}}\\sum_{n=1}^{N_{ens}} x^{b,(n)}$ the forecast ensemble mean.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad03b64e",
   "metadata": {},
   "source": [
    "We rerun the model with the perturbed initial condition and assimilate all variables every 10 time steps. We plot the truth (black) together with the output just after the DA correction (purple). \n",
    "The 1st plot shows all ensemble members, the 2nd one only the ensemble mean. Finally, we also plot \n",
    "the spread (standard deviation) of the ensemble and the root-mean-squared error (RMSE). I.e. the\n",
    "RMS between the ensemble mean of the background/analysis and the truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82c395f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create experiment with a initial state that deviates from the truth. \n",
    "exp = Experiment(x0=xguess)\n",
    "\n",
    "#Run the experiment\n",
    "exp.run()\n",
    "\n",
    "#Plot model output\n",
    "exp.plot_state()\n",
    "\n",
    "#Plot metrics as function of time. \n",
    "exp.plot_metrics(1)\n",
    "\n",
    "#Calculate and show as table the root-mean-square values over time. \n",
    "np.sqrt((exp.calculate_metrics(1)**2).mean(dim=['time'])).to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30311b95",
   "metadata": {},
   "source": [
    "1. Rerun the aforementioned experiment but now add `period_obs=2` and plot the RMSE. \n",
    "Repeat with `period_obs=20`. How does observational frequency influence the output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ea0159",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1a92af9",
   "metadata": {},
   "source": [
    "2. Try running the experiment with `x0=xguess, obs_type='1010'` and `x0=xguess, obs_type='landsea'`. \n",
    "How does the type of observations influence the output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e3f2ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5fdbbb59",
   "metadata": {},
   "source": [
    "3. Try running with an 6-member ensemble (`x0=xguess, n_ens=6`) and a 12-member ensemble (`x0=xguess, n_ens=16`). \n",
    "How does the ensemble size influence the output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce03cb4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62574c05",
   "metadata": {},
   "source": [
    "## Inflation\n",
    "\n",
    "Now let us try a more challenging setting by limiting our ensemble size from $N_{ens}=24$ to $N_{ens} = 12$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ef184b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create experiment with a initial state that deviates from the truth and with 12 ensemble members.\n",
    "exp = Experiment(x0=xguess, n_ens=12)\n",
    "\n",
    "#Run the experiment\n",
    "exp.run()\n",
    "\n",
    "#Plot the experiment's output. \n",
    "exp.plot_state()\n",
    "\n",
    "#Plot metrics\n",
    "exp.plot_metrics(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b1b0ef",
   "metadata": {},
   "source": [
    "The fundamental *ansatz* of ensemble Kalman\n",
    "filters is that the truth and each ensemble member are realisation of the same probability distribution. I.e. the\n",
    "truth can be viewed as another, but unknown, ensemble member. So, the standard deviation of the truth \n",
    "around the ensemble mean is equal to that of the ensemble members. The square-root of the former is \n",
    "the RMSE, that of the latter the ensemble spread. So, if the system is properly calibrated the expectation value\n",
    "$E[\\frac{{RMSE}^2}{\\sigma_{ens}^2}]=1$. \n",
    "\n",
    "Clearly, this is not the case in this experiment: the spread severely \n",
    "underestimates the RMSE after $t=2$. One way to solve this is to use inflation. Inflation increases the spread \n",
    "by rescaling the ensemble. The most common approach is to multiply the perturbations from the ensemble mean\n",
    "by a factor. In this practical, the an inflation factor of $\\alpha$ means that the $n$th ensemble member after inflation is\n",
    "given as $(1+\\alpha)(\\mathbf{x}^{b,(n)}-\\overline{\\mathbf{x}^{b}}) + \\overline{\\mathbf{x}^{b}}$ with $\\overline{\\mathbf{x}}$\n",
    "the ensemble mean and $\\mathbf{x}^{(n)}$ the $n$th ensemble member. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bd5feb",
   "metadata": {},
   "source": [
    "4. Try different values for `inflation` always in combination with `x0=xguess, n_ens=12`. \n",
    "What is the smallest value for which spread and RMSE approximately match?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d84729e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51f5e19b",
   "metadata": {},
   "source": [
    "5. Repeat 4. but now using `obs_grid='1010'`. What is now the smallest inflation factor for which spread and \n",
    "RMSE match?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b98c09a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "861447b4",
   "metadata": {},
   "source": [
    "Theoretically the covariance after the DA step (i.e. analysis covariance) should be given as\n",
    "\\begin{equation}\n",
    "    \\mathbf{A} = \\mathbf{B} - \\mathbf{B}\\mathbf{H}^{\\rm{T}}(\\mathbf{R}+\\mathbf{H}\\mathbf{B}\\mathbf{H}^{\\rm{T}})^{-1}\\mathbf{H} \\mathbf{B}\n",
    "\\end{equation}    \n",
    "\n",
    "For SEnKF this equality only holds on average, i.e.\n",
    "\\begin{equation}\n",
    "E[ \\frac{1}{N_{ens}-1} \\sum_{n=1}^{N_{ens}} (x^{a,(n)}-\\overline{x^{a}}) \n",
    "(x^{a,(n)}-\\overline{x^{a}})^{\\rm{T}} ] = \\mathbf{A}\n",
    "\\end{equation}\n",
    "However, single instances of \n",
    "\\begin{equation}\n",
    "\\frac{1}{N_{ens}-1} \\sum_{n=1}^{N_{ens}} (x^{a,(n)}-\\overline{x^{a}}) \n",
    "(x^{a,(n)}-\\overline{x^{a}})^{\\rm{T}}\n",
    "\\end{equation}\n",
    "might differ from $\\mathbf{A}$. \n",
    "\n",
    "The ensemble transform Kalman filter (ETKF) is an alternative to SEnKF in which no perturbations are added to the \n",
    "observations.\n",
    "Instead a linear combination of background ensemble states is sought such that \n",
    "\\begin{equation}\n",
    "\\mathbf{\\tilde{X}}^{a} = \\mathbf{\\tilde{X}}^{b} \\mathbf{W}\n",
    "\\end{equation}\n",
    "and $\\frac{1}{N_{ens}-1}\\mathbf{\\tilde{X}}^{a} \\mathbf{\\tilde{X}}^{a,\\rm{T}} = \\mathbf{A}$. \n",
    "Here the $n$th-column of $\\tilde{\\mathbf{X}}$ is $x^{(n)}-\\overline{x}$ the perturbation \n",
    "of the $n$th-ensemble member from the ensemble mean. Rewriting $\\mathbf{A}$ as \n",
    "\\begin{equation}\n",
    "\\mathbf{A} \\overset{def}{=} \\mathbf{\\tilde{X}}^{b} \\mathbf{\\tilde{A}}^{-1} \\mathbf{\\tilde{X}}^{b,\\rm{T}}\n",
    "\\end{equation}\n",
    "gives that the aforementioned equality can be achieved by setting \n",
    "\\begin{equation}\n",
    "\\mathbf{W} = \\mathbf{U} \\mathbf{\\Lambda}^{-1/2}\n",
    "\\end{equation}\n",
    "with $\\mathbf{U} \\mathbf{\\Lambda} \\mathbf{U}^{\\rm{T}}$ the singular-value decomposition of \n",
    "$\\mathbf{\\tilde{A}}$. Because it is exact up to the 2nd statistical momentum,\n",
    "we say that ETKF is a 2nd order transform.\n",
    "\n",
    "6. Repeat experiment 4. now using `da_method='ETKF'`. Does ETKF require more or less inflation than SEnKF?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21bdf04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d05db49d",
   "metadata": {},
   "source": [
    "## Localisation\n",
    "\n",
    "We are now going look into the effects of localisation on the performance of the ensemble Kalman filter. \n",
    "We will focus on model space localisation in the stochastic EnKF. The domain\n",
    "localisation needed for ETKF is very slow without parallelisation, so we are not going to use ETKF in this section.\n",
    "\n",
    "First, we create a class to hold the default settings for the experiments in this section. These defaults are \n",
    "slightly different from the ones in the previous section. Here we will be using the\n",
    "stochastic ensemble Kalman filter with 12 ensemble members to assimilate every 2nd point, every 2nd time step using \n",
    "an observational error standard deviation of $\\sigma_{obs} = \\sqrt{2}$, an inflation factor of 0.1. By \n",
    "default localisation is not yet activated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450a9c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalisationExperiment(Experiment):\n",
    "    \"\"\" \n",
    "    Class that overwrites the defaults for period_obs, inflation and obs_grid in the Experiment class.     \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        defaults = {'period_obs' : 2,\n",
    "                   'inflation' : 0.05,\n",
    "                   'obs_grid' : '1010',\n",
    "                   'n_ens' : 12}\n",
    "        defaults = {**defaults, **kwargs}\n",
    "        super().__init__(**defaults)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcbd85c",
   "metadata": {},
   "source": [
    "As reference we run the experiment with the default settings and without localisation using the initial guess\n",
    "that deviates from the truth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3603d916",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the experiment\n",
    "exp = LocalisationExperiment(x0 = xguess)\n",
    "exp.run()\n",
    "\n",
    "#Plot state and error metrics. \n",
    "exp.plot_state()\n",
    "exp.plot_metrics(1)\n",
    "\n",
    "#Calculate the root-mean-square values over time. \n",
    "np.sqrt((exp.calculate_metrics(1)**2).mean(dim=['time'])).to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f68bc88",
   "metadata": {},
   "source": [
    "We see that the RMSE grows considerably larger than the ensemble spread. Here we will \n",
    "test whether localisation can fix this. \n",
    "\n",
    "With domain localisation the stochastic ensemble Kalman filter correction for ensemble member $n$ becomes \n",
    "\\begin{equation}\n",
    "x^{a,(n)} = x^{b,(n)} \n",
    "+ \\mathbf{L} \\circ (\\mathbf{B} \\mathbf{H}^{\\rm{T}}) (\\mathbf{R}+\\mathbf{H} \\mathbf{L} \\circ (\\mathbf{B}\\mathbf{H}^{\\rm{T}}))^{-1}\n",
    "(y - \\mathbf{H} x^{b,(n)} + \\mathbf{R}^{\\frac{1}{2}}\\epsilon)\n",
    "\\end{equation}\n",
    "with $\\circ$ the Hadamard, or elementwise, product and $\\mathbf{L}$ the localisation matrix.  \n",
    "\n",
    "To illustrate the effect of ensemble size and localisation we first run the experiment with a large ensemble \n",
    "of 256 members and plot the intial background error covariance together with its eigenvalues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65076948",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the experiment\n",
    "exp = LocalisationExperiment(x0 = xguess, n_ens=256)\n",
    "exp.run()\n",
    "\n",
    "#Plot the covariance at t=0 together with its eigenvalues. \n",
    "P = np.cov(exp.Xb[:,:,0])\n",
    "plot_LocMatrix(P, name='true covariance')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ba6988",
   "metadata": {},
   "source": [
    "7. Plot the initial background covariance, but now from an ensemble with `n_ens=12` members. Is the background error\n",
    "covariance and its spectrum the same as the one obtained from 256-member ensemble?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a513fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cebca2",
   "metadata": {},
   "source": [
    "In order for our 12-member ensemble covariance to give a closer approximation to the true covariance,\n",
    "we are going to apply localisation. The localisation weights every element in the background error covariance matrix. Elements on the diagonal are multiplied\n",
    "by 1, while those off the diagonal are multiplied with values $<1$ that decrease as the elements are located further\n",
    "from the diagonal. I.e. we are going to look at $\\mathbf{L} \\circ \\mathbf{B}$, the domain localisation \n",
    "case in which the observation operator $\\mathbf{H}$ is the identity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78632dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the experiment\n",
    "exp = LocalisationExperiment(x0 = xguess, loc_radius=2.0)\n",
    "exp.run()\n",
    "\n",
    "#Plot the localisation matrix. \n",
    "plot_LocMatrix(np.array(exp.L_x), name='localisation')\n",
    "\n",
    "#Localise background error matrix by taking the Hadamard product with the localisation matrix. \n",
    "P = np.cov(exp.Xb[:,:,0]) * exp.L_x\n",
    "\n",
    "#Plot the localised background error covariance matrix. \n",
    "plot_LocMatrix(P, name='localised covariance')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a12db1",
   "metadata": {},
   "source": [
    "8. Compare the localised covariance with the one from the 256-member ensemble. What is the effect of localisation on the\n",
    "the covariance matrix and what is the effect on its spectrum?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58af320",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5dc11154",
   "metadata": {},
   "source": [
    "9. Run the using settings `x0 = xguess, loc_radius=2.0` like in the previous experiment. \n",
    "Plot the RMSE using `exp.plot_metrics()`. Does the application of localisation reduce the RMSE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e97b0c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06651a43",
   "metadata": {},
   "source": [
    "10. Repeat experiment 9 `for loc_radius in [None, 1.0, 2.0, 5.0, 9.0]`. Which localisation radius gives you the \n",
    "lowest RMSE? At which `loc_radius` do you get the lowest RMSE if you also pass `n_ens=24` as argument to \n",
    "`LocalisationExperiment`? What do you conclude about the nature of the (qualitative) relation between \n",
    "the optimal localisation radius and the ensemble size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ceb4ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53bf46e8",
   "metadata": {},
   "source": [
    "11. In the previous section, we explained that the RMSE and ensemble spread should, on average,\n",
    "be equal. We also showed that ensemble inflation can be used to achieve this. For `n_ens=12` and \n",
    "`for loc_radius in [None, 2, 5]` find the smallest nonnegative value of `inflation` for which RMSE and \n",
    "ensemble spread are approximately equal over the simulation period. How does the localisation radius \n",
    "impact the need for inflation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9c462b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2837e00",
   "metadata": {},
   "source": [
    "12. Time permitting repeat experiment 10 using `obs_grid = 'landsea'`. How does the design of the observation network impact\n",
    "the need for localisation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dd83fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
